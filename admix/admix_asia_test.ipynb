{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# admixでの日本、韓国、中国パネルをテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# check if file exits\n",
    "def check_file(file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        print('Cannot find the file \\'' + file_name + '\\'!\\n')\n",
    "        exit()\n",
    "\n",
    "# convert 23andme raw data\n",
    "def twenty_three_and_me(data_file_name):\n",
    "    check_file(data_file_name)\n",
    "    processed_data = {}\n",
    "    with open(data_file_name, 'r') as data:\n",
    "        data = csv.reader(data, delimiter='\\t')\n",
    "        for row in data:\n",
    "            # make sure the genotype is valid\n",
    "            if len(row) == 4 and row[-1][-1] in ['A', 'T', 'G', 'C']:\n",
    "                processed_data[row[0]] = row[-1]\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def read_raw_data(data_format, data_file_name=None):\n",
    "    if data_format == \"23andme\":\n",
    "        if not data_file_name is None:\n",
    "            return twenty_three_and_me(data_file_name)\n",
    "        else:\n",
    "            print(\"Data file not set!\")\n",
    "            exit()\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Data format does not exist!\")\n",
    "        exit()\n",
    "        return None\n",
    "    \n",
    "\n",
    "# all models\n",
    "def models():\n",
    "    return ['EastAsia3',\n",
    "            'K7b',\n",
    "            'K12b',]\n",
    "\n",
    "\n",
    "def populations(model):\n",
    "    if model == 'K7b':\n",
    "        return [('South Asian','南亚'),\n",
    "                ('West Asian','西亚'),\n",
    "                ('Siberian','西伯利亚'),\n",
    "                ('African','非洲'),\n",
    "                ('Southern','地中海－中东'),\n",
    "                ('Atlantic Baltic','大西洋波罗的海'),\n",
    "                ('East Asian','东亚')]\n",
    "    elif model == 'K12b':\n",
    "        return [('Gedrosia','格德罗西亚'),\n",
    "                ('Siberian','西伯利亚'),\n",
    "                ('Northwest African','西北非'),\n",
    "                ('Southeast Asian','东南亚'),\n",
    "                ('Atlantic Med','大西洋地中海'),\n",
    "                ('North European','北欧'),\n",
    "                ('South Asian','南亚'),\n",
    "                ('East African','东非'),\n",
    "                ('Southwest Asian','西南亚'),\n",
    "                ('East Asian','东亚'),\n",
    "                ('Caucasus','高加索'),\n",
    "                ('Sub Saharan','撒哈拉以南非洲')]\n",
    "    elif model == 'EastAsia3':\n",
    "        return [(\"Han\",'中国'),\n",
    "                ('Japanese','日本'),\n",
    "                ('Korean','韓国')]\n",
    "\n",
    "# number of populations in all models\n",
    "def n_populations(model):\n",
    "    return len(populations(model))\n",
    "\n",
    "\n",
    "# model alleles file names\n",
    "def snp_file_name(model):\n",
    "    return model + \".alleles\"\n",
    "\n",
    "\n",
    "# model frequency matrix file names\n",
    "def frequency_file_name(model):\n",
    "    return model + \".\" + str(n_populations(model)) + \".F\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert alleles information of a model to a dict\n",
    "def read_model(model):\n",
    "    # obtain model file names\n",
    "    snp_file_name = snp_file_name(model)\n",
    "    frequency_file_name = frequency_file_name(model)\n",
    "\n",
    "    # read SNPs\n",
    "    snp = []\n",
    "    minor_alleles = []\n",
    "    major_alleles = []\n",
    "\n",
    "    with open(\n",
    "            os.path.join(os.path.dirname(__file__), 'data/' + snp_file_name),\n",
    "            'r') as snp_file:\n",
    "        snp_file = csv.reader(snp_file, delimiter=' ')\n",
    "        for row in snp_file:\n",
    "            snp.append(row[0])\n",
    "            minor_alleles.append(row[1])\n",
    "            major_alleles.append(row[2])\n",
    "\n",
    "    # read frequency matrix\n",
    "    frequency = []\n",
    "    with open(\n",
    "            os.path.join(\n",
    "                os.path.dirname(__file__), 'data/' + frequency_file_name),\n",
    "            'r') as frequency_file:\n",
    "        frequency_file = csv.reader(frequency_file, delimiter=' ')\n",
    "        for row in frequency_file:\n",
    "            frequency.append([float(f) for f in row])\n",
    "\n",
    "    return np.array(snp), np.array(minor_alleles), np.array(\n",
    "        major_alleles), np.array(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"EastAsia3\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1958978c2497e964c1b57907faa928d5d036f1ea3b7764fa82b830495842c4aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
